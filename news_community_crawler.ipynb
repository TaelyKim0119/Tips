{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 뉴스 및 커뮤니티 데이터 수집기\n",
        "\n",
        "이 노트북은 다음 기능들을 제공합니다:\n",
        "\n",
        "1. 구글 뉴스에서 원하는 키워드로 뉴스 기사 수집\n",
        "2. 네이버 뉴스에서 뉴스 기사 수집\n",
        "3. 디시인사이드, 네이버 카페 등 커뮤니티 게시글 수집\n",
        "4. 수집된 텍스트에서 주요 개체(인물, 조직 등) 추출\n",
        "5. 결과를 CSV 파일로 저장\n",
        "\n",
        "## 사용 방법\n",
        "1. 키워드를 설정하여 원하는 정보 검색\n",
        "2. 수집 기간 설정 가능\n",
        "3. 원하는 커뮤니티 선택 가능\n",
        "4. 결과물은 CSV 파일로 저장됨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting GoogleNewsNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading GoogleNews-1.6.15-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers in c:\\users\\speec\\anaconda3\\lib\\site-packages (4.55.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\speec\\anaconda3\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: requests in c:\\users\\speec\\anaconda3\\lib\\site-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\speec\\anaconda3\\lib\\site-packages (4.12.2)\n",
            "Requirement already satisfied: selenium in c:\\users\\speec\\anaconda3\\lib\\site-packages (4.33.0)\n",
            "Requirement already satisfied: webdriver-manager in c:\\users\\speec\\anaconda3\\lib\\site-packages (4.0.2)\n",
            "Collecting fugashi[unidic]\n",
            "  Downloading fugashi-1.5.1-cp312-cp312-win_amd64.whl.metadata (7.5 kB)\n",
            "Collecting dateparser (from GoogleNews)\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\speec\\anaconda3\\lib\\site-packages (from GoogleNews) (2.9.0.post0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from newspaper3k) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.10)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from newspaper3k) (5.1.2)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
            "     ---------------------------- ----------- 5.2/7.4 MB 26.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 7.4/7.4 MB 21.8 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: filelock in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
            "Collecting unidic (from fugashi[unidic])\n",
            "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: trio~=0.30.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\speec\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.0.0)\n",
            "Requirement already satisfied: six in c:\\users\\speec\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in c:\\users\\speec\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
            "Requirement already satisfied: click in c:\\users\\speec\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\speec\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\speec\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\speec\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in c:\\users\\speec\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzlocal>=0.2 (from dateparser->GoogleNews)\n",
            "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting wasabi<1.0.0,>=0.6.0 (from unidic->fugashi[unidic])\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting plac<2.0.0,>=1.1.3 (from unidic->fugashi[unidic])\n",
            "  Downloading plac-1.4.5-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pycparser in c:\\users\\speec\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\speec\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
            "Downloading GoogleNews-1.6.15-py3-none-any.whl (8.8 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading fugashi-1.5.1-cp312-cp312-win_amd64.whl (513 kB)\n",
            "Downloading plac-1.4.5-py2.py3-none-any.whl (22 kB)\n",
            "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
            "Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, unidic\n",
            "  Building wheel for tinysegmenter (setup.py): started\n",
            "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13566 sha256=bc398eb4bc36f86a22f48b545828f0258bd956f6b72851320ff085af14a33cd1\n",
            "  Stored in directory: c:\\users\\speec\\appdata\\local\\pip\\cache\\wheels\\a5\\91\\9f\\00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
            "  Building wheel for feedfinder2 (setup.py): started\n",
            "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3358 sha256=0721c4dc4903392086905f74fad54bfb61197a720ca9f316ef89599103c64e2f\n",
            "  Stored in directory: c:\\users\\speec\\appdata\\local\\pip\\cache\\wheels\\9f\\9f\\fb\\364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
            "  Building wheel for jieba3k (setup.py): started\n",
            "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398386 sha256=20c4b4a1c3c6e7144683178ac8b1abc1f7e7d9d77d7d590067e88bfd1f654a2a\n",
            "  Stored in directory: c:\\users\\speec\\appdata\\local\\pip\\cache\\wheels\\26\\72\\f7\\fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
            "  Building wheel for unidic (setup.py): started\n",
            "  Building wheel for unidic (setup.py): finished with status 'done'\n",
            "  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7431 sha256=bd6c1c29ce2562663109db79d22633be2bde06fc97bfa14314f3997eb7d61cbd\n",
            "  Stored in directory: c:\\users\\speec\\appdata\\local\\pip\\cache\\wheels\\cb\\04\\a2\\659428f84ed1fa7257f8efb544b36043dd37a7c419e8ca711a\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k unidic\n",
            "Installing collected packages: wasabi, tinysegmenter, pytz, plac, jieba3k, tzlocal, fugashi, unidic, feedfinder2, dateparser, GoogleNews, newspaper3k\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.1\n",
            "    Uninstalling pytz-2024.1:\n",
            "      Successfully uninstalled pytz-2024.1\n",
            "Successfully installed GoogleNews-1.6.15 dateparser-1.2.2 feedfinder2-0.0.4 fugashi-1.5.1 jieba3k-0.35.1 newspaper3k-0.2.8 plac-1.4.5 pytz-2025.2 tinysegmenter-0.3 tzlocal-5.3.1 unidic-1.1.0 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "%pip install GoogleNews newspaper3k transformers fugashi[unidic] pandas requests beautifulsoup4 selenium webdriver-manager\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "from GoogleNews import GoogleNews\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime, timedelta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NewsCrawler:\n",
        "    def __init__(self, query, period='6m', lang='ko'):\n",
        "        self.query = query\n",
        "        self.period = period\n",
        "        self.lang = lang\n",
        "        \n",
        "    def collect_google_news(self):\n",
        "        \"\"\"구글 뉴스에서 기사 수집\"\"\"\n",
        "        googlenews = GoogleNews(lang=self.lang)\n",
        "        googlenews.set_period(self.period)\n",
        "        googlenews.search(self.query)\n",
        "        return googlenews.result()\n",
        "    \n",
        "    def collect_naver_news(self, pages=5):\n",
        "        \"\"\"네이버 뉴스에서 기사 수집\"\"\"\n",
        "        articles = []\n",
        "        \n",
        "        for page in range(1, pages + 1):\n",
        "            url = f\"https://search.naver.com/search.naver?where=news&query={self.query}&start={page*10-9}\"\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            \n",
        "            news_items = soup.select('.news_wrap')\n",
        "            for item in news_items:\n",
        "                try:\n",
        "                    title = item.select_one('.news_tit').text\n",
        "                    link = item.select_one('.news_tit')['href']\n",
        "                    desc = item.select_one('.news_dsc').text if item.select_one('.news_dsc') else \"\"\n",
        "                    \n",
        "                    articles.append({\n",
        "                        'title': title,\n",
        "                        'description': desc,\n",
        "                        'link': link\n",
        "                    })\n",
        "                except:\n",
        "                    continue\n",
        "                    \n",
        "        return articles\n",
        "    \n",
        "    def parse_article(self, url):\n",
        "        \"\"\"기사 URL에서 본문 추출\"\"\"\n",
        "        try:\n",
        "            article = Article(url, language=self.lang)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            return {\n",
        "                \"title\": article.title,\n",
        "                \"text\": article.text,\n",
        "                \"url\": url\n",
        "            }\n",
        "        except:\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CommunityCrawler:\n",
        "    def __init__(self):\n",
        "        self.driver = None\n",
        "        \n",
        "    def setup_driver(self):\n",
        "        \"\"\"셀레니움 드라이버 설정\"\"\"\n",
        "        if not self.driver:\n",
        "            service = Service(ChromeDriverManager().install())\n",
        "            self.driver = webdriver.Chrome(service=service)\n",
        "            \n",
        "    def close_driver(self):\n",
        "        \"\"\"드라이버 종료\"\"\"\n",
        "        if self.driver:\n",
        "            self.driver.quit()\n",
        "            self.driver = None\n",
        "            \n",
        "    def collect_dcinside(self, gallery_id, pages=3):\n",
        "        \"\"\"디시인사이드 갤러리 게시글 수집\"\"\"\n",
        "        self.setup_driver()\n",
        "        posts = []\n",
        "        \n",
        "        for page in range(1, pages + 1):\n",
        "            url = f\"https://gall.dcinside.com/board/lists/?id={gallery_id}&page={page}\"\n",
        "            self.driver.get(url)\n",
        "            time.sleep(2)\n",
        "            \n",
        "            # 게시글 목록 추출\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            items = soup.select('.us-post')\n",
        "            \n",
        "            for item in items:\n",
        "                try:\n",
        "                    title = item.select_one('.gall_tit').text.strip()\n",
        "                    link = item.select_one('.gall_tit a')['href']\n",
        "                    date = item.select_one('.gall_date')['title']\n",
        "                    \n",
        "                    # 게시글 내용 가져오기\n",
        "                    self.driver.get(f\"https://gall.dcinside.com{link}\")\n",
        "                    time.sleep(1)\n",
        "                    post_soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "                    content = post_soup.select_one('.writing_view_box').text.strip()\n",
        "                    \n",
        "                    posts.append({\n",
        "                        'title': title,\n",
        "                        'content': content,\n",
        "                        'date': date,\n",
        "                        'url': f\"https://gall.dcinside.com{link}\"\n",
        "                    })\n",
        "                except:\n",
        "                    continue\n",
        "                    \n",
        "        return posts\n",
        "    \n",
        "    def collect_naver_cafe(self, cafe_id, menu_id, query, pages=3):\n",
        "        \"\"\"네이버 카페 게시글 수집\"\"\"\n",
        "        self.setup_driver()\n",
        "        posts = []\n",
        "        \n",
        "        for page in range(1, pages + 1):\n",
        "            url = f\"https://cafe.naver.com/{cafe_id}?iframe_url=/ArticleSearchList.nhn?search.clubid={cafe_id}&search.menuid={menu_id}&search.media=0&search.searchdate=all&search.defaultValue=1&search.exact=&search.include=&userDisplay=15&search.exclude=&search.option=0&search.sortBy=date&search.searchBy=0&search.searchBlockYn=0&search.includeAll=&search.query={query}&search.viewtype=title&search.page={page}\"\n",
        "            self.driver.get(url)\n",
        "            time.sleep(2)\n",
        "            \n",
        "            # iframe 전환\n",
        "            self.driver.switch_to.frame('cafe_main')\n",
        "            \n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            items = soup.select('.article-board tr:not(.notice)')\n",
        "            \n",
        "            for item in items:\n",
        "                try:\n",
        "                    title = item.select_one('.article').text.strip()\n",
        "                    link = item.select_one('.article')['href']\n",
        "                    date = item.select_one('.date').text.strip()\n",
        "                    \n",
        "                    posts.append({\n",
        "                        'title': title,\n",
        "                        'date': date,\n",
        "                        'url': f\"https://cafe.naver.com{link}\"\n",
        "                    })\n",
        "                except:\n",
        "                    continue\n",
        "                    \n",
        "        return posts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities(texts, model_name=\"jinmang2/klue-ner\"):\n",
        "    \"\"\"텍스트에서 개체명(인물, 조직 등) 추출\"\"\"\n",
        "    ner = pipeline(\"ner\", model=model_name, tokenizer=model_name)\n",
        "    \n",
        "    entities = []\n",
        "    for text in texts:\n",
        "        try:\n",
        "            ner_results = ner(text)\n",
        "            for ent in ner_results:\n",
        "                if ent['entity'] in [\"PER\", \"ORG\"]:  # 인물/조직만 필터링\n",
        "                    entities.append({\n",
        "                        \"name\": ent['word'],\n",
        "                        \"type\": ent['entity'],\n",
        "                        \"context\": text[:200]  # 앞 200자 문맥\n",
        "                    })\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "    return entities\n",
        "\n",
        "def save_to_csv(data, filename, encoding=\"utf-8-sig\"):\n",
        "    \"\"\"데이터를 CSV 파일로 저장\"\"\"\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False, encoding=encoding)\n",
        "    print(f\"파일 저장 완료: {filename}\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검색어: 검색하고 싶은 키워드\n",
            "네이버 뉴스 수집 시작...\n",
            "네이버 뉴스 0개 수집 완료\n",
            "⚠️ 네이버 뉴스가 수집되지 않았습니다. 다음 사항을 확인하세요:\n",
            "1. 검색어가 올바른지 확인\n",
            "2. 네트워크 연결 상태 확인\n",
            "3. 네이버 뉴스 사이트 접근 가능 여부 확인\n",
            "\n",
            "테스트용 검색어 '삼성전자'로 재시도...\n",
            "테스트 결과: 0개 수집\n",
            "유튜브 검색 시작: 검색하고 싶은 키워드\n",
            "유튜브 크롤링 전체 오류: name 'quote' is not defined\n",
            "유튜브 영상 0개 수집 완료\n",
            "⚠️ 유튜브 영상이 수집되지 않았습니다. 다음 사항을 확인하세요:\n",
            "1. 검색어가 영어나 한글로 올바르게 입력되었는지 확인\n",
            "2. Chrome 드라이버가 올바르게 설치되었는지 확인\n",
            "3. 유튜브 사이트 접근 가능 여부 확인\n",
            "\n",
            "수집된 데이터 처리 시작...\n",
            "네이버 뉴스: 0개\n",
            "유튜브 영상: 0개\n",
            "성공적으로 처리된 뉴스 기사: 0개\n",
            "전체 처리된 텍스트 수: 0개\n",
            "⚠️ 처리할 텍스트가 없어 개체명 추출을 건너뜁니다.\n",
            "⚠️ 저장할 데이터가 없습니다.\n",
            "\n",
            "==================================================\n",
            "=== 수집 결과 요약 ===\n",
            "==================================================\n",
            "검색어: 검색하고 싶은 키워드\n",
            "네이버 뉴스: 0개\n",
            "유튜브 영상: 0개\n",
            "추출된 개체명: 0개\n",
            "전체 텍스트 수: 0개\n",
            "성공적으로 처리된 기사: 0개\n",
            "\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "⚠️ 수집된 데이터가 없습니다!\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "문제 해결 방법:\n",
            "1. 검색어를 더 일반적인 키워드로 변경해보세요\n",
            "2. 네트워크 연결을 확인하세요\n",
            "3. 웹드라이버(Chrome)가 제대로 설치되었는지 확인하세요\n",
            "4. 웹사이트 접근이 차단되지 않았는지 확인하세요\n",
            "5. 시간을 두고 다시 시도해보세요\n",
            "\n",
            "=== 개체명 통계 ===\n",
            "추출된 개체명이 없습니다.\n"
          ]
        }
      ],
      "source": [
        "# 사용 예제 (디버깅 개선 버전)\n",
        "\n",
        "# 1. 검색어 설정\n",
        "query = \"검색하고 싶은 키워드\"  # 예: \"SKT 유심 유출\"\n",
        "print(f\"검색어: {query}\")\n",
        "\n",
        "# 2. 뉴스 수집 (디버깅 추가)\n",
        "news_crawler = NewsCrawler(query)\n",
        "\n",
        "# 네이버 뉴스 수집\n",
        "print(\"네이버 뉴스 수집 시작...\")\n",
        "naver_news = news_crawler.collect_naver_news(pages=5)\n",
        "print(f\"네이버 뉴스 {len(naver_news)}개 수집 완료\")\n",
        "\n",
        "# 수집된 뉴스가 없는 경우 원인 확인\n",
        "if len(naver_news) == 0:\n",
        "    print(\"⚠️ 네이버 뉴스가 수집되지 않았습니다. 다음 사항을 확인하세요:\")\n",
        "    print(\"1. 검색어가 올바른지 확인\")\n",
        "    print(\"2. 네트워크 연결 상태 확인\")\n",
        "    print(\"3. 네이버 뉴스 사이트 접근 가능 여부 확인\")\n",
        "    \n",
        "    # 테스트용 검색어로 재시도\n",
        "    test_query = \"삼성전자\"\n",
        "    print(f\"\\n테스트용 검색어 '{test_query}'로 재시도...\")\n",
        "    test_crawler = NewsCrawler(test_query)\n",
        "    test_news = test_crawler.collect_naver_news(pages=1)\n",
        "    print(f\"테스트 결과: {len(test_news)}개 수집\")\n",
        "\n",
        "# 3. 유튜브 영상 정보 수집 (디버깅 추가)\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "def collect_youtube_videos(query, max_results=20):\n",
        "    \"\"\"유튜브에서 검색어로 영상 정보 수집 (디버깅 개선)\"\"\"\n",
        "    print(f\"유튜브 검색 시작: {query}\")\n",
        "    driver = webdriver.Chrome()\n",
        "    youtube_data = []\n",
        "    \n",
        "    try:\n",
        "        # 유튜브 검색\n",
        "        search_url = f\"https://www.youtube.com/results?search_query={quote(query)}\"\n",
        "        print(f\"유튜브 URL: {search_url}\")\n",
        "        driver.get(search_url)\n",
        "        time.sleep(3)\n",
        "        \n",
        "        # 페이지 로드 확인\n",
        "        page_title = driver.title\n",
        "        print(f\"페이지 제목: {page_title}\")\n",
        "        \n",
        "        # 스크롤하여 더 많은 영상 로드\n",
        "        print(\"페이지 스크롤 중...\")\n",
        "        for i in range(3):\n",
        "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "            time.sleep(2)\n",
        "            print(f\"스크롤 {i+1}/3 완료\")\n",
        "        \n",
        "        # 영상 정보 추출\n",
        "        print(\"영상 요소 검색 중...\")\n",
        "        videos = driver.find_elements(By.CSS_SELECTOR, \"div#contents ytd-video-renderer\")\n",
        "        print(f\"발견된 영상 요소 수: {len(videos)}\")\n",
        "        \n",
        "        if len(videos) == 0:\n",
        "            print(\"⚠️ 영상 요소를 찾을 수 없습니다. CSS 선택자를 확인합니다...\")\n",
        "            # 대안 선택자 시도\n",
        "            alternative_videos = driver.find_elements(By.CSS_SELECTOR, \"ytd-video-renderer\")\n",
        "            print(f\"대안 선택자로 발견된 영상: {len(alternative_videos)}\")\n",
        "            videos = alternative_videos\n",
        "        \n",
        "        for i, video in enumerate(videos[:max_results]):\n",
        "            try:\n",
        "                print(f\"영상 {i+1} 처리 중...\")\n",
        "                title_element = video.find_element(By.CSS_SELECTOR, \"a#video-title\")\n",
        "                title = title_element.get_attribute(\"title\")\n",
        "                video_url = title_element.get_attribute(\"href\")\n",
        "                \n",
        "                if not title or not video_url:\n",
        "                    print(f\"영상 {i+1}: 제목 또는 URL이 없음\")\n",
        "                    continue\n",
        "                \n",
        "                # 채널명\n",
        "                try:\n",
        "                    channel = video.find_element(By.CSS_SELECTOR, \"a.yt-simple-endpoint.style-scope.yt-formatted-string\").text\n",
        "                except:\n",
        "                    channel = \"채널명 없음\"\n",
        "                \n",
        "                # 조회수와 업로드 시간\n",
        "                try:\n",
        "                    meta_info = video.find_elements(By.CSS_SELECTOR, \"span.style-scope.ytd-video-meta-block\")\n",
        "                    views = meta_info[0].text if len(meta_info) > 0 else \"정보 없음\"\n",
        "                    upload_time = meta_info[1].text if len(meta_info) > 1 else \"정보 없음\"\n",
        "                except:\n",
        "                    views = \"정보 없음\"\n",
        "                    upload_time = \"정보 없음\"\n",
        "                \n",
        "                youtube_data.append({\n",
        "                    'title': title,\n",
        "                    'channel': channel,\n",
        "                    'views': views,\n",
        "                    'upload_time': upload_time,\n",
        "                    'url': video_url,\n",
        "                    'platform': 'YouTube'\n",
        "                })\n",
        "                \n",
        "                print(f\"영상 {i+1} 수집 완료: {title[:50]}...\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"영상 {i+1} 처리 중 오류: {e}\")\n",
        "                continue\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"유튜브 크롤링 전체 오류: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "    \n",
        "    return youtube_data\n",
        "\n",
        "# 유튜브 영상 정보 수집\n",
        "youtube_videos = collect_youtube_videos(query, max_results=30)\n",
        "print(f\"유튜브 영상 {len(youtube_videos)}개 수집 완료\")\n",
        "\n",
        "# 수집된 유튜브 영상이 없는 경우 원인 확인\n",
        "if len(youtube_videos) == 0:\n",
        "    print(\"⚠️ 유튜브 영상이 수집되지 않았습니다. 다음 사항을 확인하세요:\")\n",
        "    print(\"1. 검색어가 영어나 한글로 올바르게 입력되었는지 확인\")\n",
        "    print(\"2. Chrome 드라이버가 올바르게 설치되었는지 확인\")\n",
        "    print(\"3. 유튜브 사이트 접근 가능 여부 확인\")\n",
        "\n",
        "# 4. 수집된 데이터 통합 및 처리\n",
        "all_texts = []\n",
        "all_data = []\n",
        "\n",
        "print(f\"\\n수집된 데이터 처리 시작...\")\n",
        "print(f\"네이버 뉴스: {len(naver_news)}개\")\n",
        "print(f\"유튜브 영상: {len(youtube_videos)}개\")\n",
        "\n",
        "# 네이버 뉴스 데이터 처리\n",
        "successful_articles = 0\n",
        "for i, article in enumerate(naver_news):\n",
        "    print(f\"뉴스 기사 {i+1}/{len(naver_news)} 처리 중...\")\n",
        "    if 'link' in article:\n",
        "        parsed = news_crawler.parse_article(article['link'])\n",
        "        if parsed and parsed['text']:\n",
        "            all_texts.append(parsed['text'])\n",
        "            all_data.append({\n",
        "                'type': '네이버뉴스',\n",
        "                'title': article.get('title', ''),\n",
        "                'content': parsed['text'][:500],  # 앞 500자만 저장\n",
        "                'date': article.get('date', ''),\n",
        "                'url': article.get('link', ''),\n",
        "                'source': article.get('source', '')\n",
        "            })\n",
        "            successful_articles += 1\n",
        "\n",
        "print(f\"성공적으로 처리된 뉴스 기사: {successful_articles}개\")\n",
        "\n",
        "# 유튜브 영상 데이터 처리 (제목과 채널명을 텍스트로 활용)\n",
        "for i, video in enumerate(youtube_videos):\n",
        "    print(f\"유튜브 영상 {i+1}/{len(youtube_videos)} 처리 중...\")\n",
        "    video_text = f\"{video['title']} {video['channel']}\"\n",
        "    all_texts.append(video_text)\n",
        "    all_data.append({\n",
        "        'type': '유튜브',\n",
        "        'title': video['title'],\n",
        "        'content': f\"채널: {video['channel']}, 조회수: {video['views']}, 업로드: {video['upload_time']}\",\n",
        "        'date': video['upload_time'],\n",
        "        'url': video['url'],\n",
        "        'source': video['channel']\n",
        "    })\n",
        "\n",
        "print(f\"전체 처리된 텍스트 수: {len(all_texts)}개\")\n",
        "\n",
        "# 5. 개체명 추출 (수집된 데이터가 있을 때만)\n",
        "entities = []\n",
        "if len(all_texts) > 0:\n",
        "    print(\"개체명 추출 중...\")\n",
        "    try:\n",
        "        # 더 안정적인 NER 모델 사용\n",
        "        model_alternatives = [\n",
        "            \"klue/roberta-large-ner\",  # KLUE 공식 모델\n",
        "            \"monologg/kobert-ner\",     # KoBERT NER\n",
        "            \"jinmang2/korean-ner\"      # 대안 모델\n",
        "        ]\n",
        "        \n",
        "        for model_name in model_alternatives:\n",
        "            try:\n",
        "                print(f\"NER 모델 시도: {model_name}\")\n",
        "                entities = extract_entities(all_texts, model_name)\n",
        "                print(f\"개체명 추출 성공: {len(entities)}개\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"모델 {model_name} 실패: {e}\")\n",
        "                continue\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"개체명 추출 실패: {e}\")\n",
        "        print(\"개체명 추출 없이 계속 진행합니다.\")\n",
        "else:\n",
        "    print(\"⚠️ 처리할 텍스트가 없어 개체명 추출을 건너뜁니다.\")\n",
        "\n",
        "# 6. 결과 저장 (데이터가 있을 때만)\n",
        "if len(all_data) > 0:\n",
        "    print(\"결과 파일 저장 중...\")\n",
        "    # 전체 수집 데이터 저장\n",
        "    save_to_csv(all_data, f\"{query}_수집데이터.csv\")\n",
        "    \n",
        "    # 네이버 뉴스만 따로 저장\n",
        "    if len(naver_news) > 0:\n",
        "        save_to_csv(naver_news, f\"{query}_네이버뉴스.csv\")\n",
        "    \n",
        "    # 유튜브 영상만 따로 저장\n",
        "    if len(youtube_videos) > 0:\n",
        "        save_to_csv(youtube_videos, f\"{query}_유튜브영상.csv\")\n",
        "    \n",
        "    # 추출된 개체명 저장\n",
        "    if len(entities) > 0:\n",
        "        save_to_csv(entities, f\"{query}_개체명.csv\")\n",
        "else:\n",
        "    print(\"⚠️ 저장할 데이터가 없습니다.\")\n",
        "\n",
        "# 7. 수집 결과 요약\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"=== 수집 결과 요약 ===\")\n",
        "print(\"=\"*50)\n",
        "print(f\"검색어: {query}\")\n",
        "print(f\"네이버 뉴스: {len(naver_news)}개\")\n",
        "print(f\"유튜브 영상: {len(youtube_videos)}개\")\n",
        "print(f\"추출된 개체명: {len(entities)}개\")\n",
        "print(f\"전체 텍스트 수: {len(all_texts)}개\")\n",
        "print(f\"성공적으로 처리된 기사: {successful_articles}개\")\n",
        "\n",
        "# 수집 결과가 0인 경우 문제 진단\n",
        "if len(naver_news) == 0 and len(youtube_videos) == 0:\n",
        "    print(\"\\n\" + \"!\"*50)\n",
        "    print(\"⚠️ 수집된 데이터가 없습니다!\")\n",
        "    print(\"!\"*50)\n",
        "    print(\"문제 해결 방법:\")\n",
        "    print(\"1. 검색어를 더 일반적인 키워드로 변경해보세요\")\n",
        "    print(\"2. 네트워크 연결을 확인하세요\")\n",
        "    print(\"3. 웹드라이버(Chrome)가 제대로 설치되었는지 확인하세요\")\n",
        "    print(\"4. 웹사이트 접근이 차단되지 않았는지 확인하세요\")\n",
        "    print(\"5. 시간을 두고 다시 시도해보세요\")\n",
        "\n",
        "# 개체명 유형별 통계\n",
        "if len(entities) > 0:\n",
        "    entity_types = {}\n",
        "    for entity in entities:\n",
        "        entity_type = entity.get('type', 'Unknown')\n",
        "        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
        "\n",
        "    print(\"\\n=== 개체명 유형별 통계 ===\")\n",
        "    for etype, count in entity_types.items():\n",
        "        print(f\"{etype}: {count}개\")\n",
        "else:\n",
        "    print(\"\\n=== 개체명 통계 ===\")\n",
        "    print(\"추출된 개체명이 없습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 사용 시 주의사항\n",
        "\n",
        "1. **웹사이트 정책 준수**\n",
        "   - 각 웹사이트의 이용약관과 로봇 정책을 확인하고 준수해야 합니다.\n",
        "   - 과도한 요청은 IP 차단의 원인이 될 수 있습니다.\n",
        "\n",
        "2. **시간 간격 설정**\n",
        "   - 웹사이트 서버에 부담을 주지 않도록 적절한 시간 간격을 두고 크롤링합니다.\n",
        "   - 현재 코드에는 기본적인 대기 시간이 설정되어 있습니다.\n",
        "\n",
        "3. **로그인이 필요한 경우**\n",
        "   - 네이버 카페 등 로그인이 필요한 사이트는 별도의 로그인 처리가 필요할 수 있습니다.\n",
        "   - Selenium을 사용하여 수동으로 로그인 후 크롤링을 진행하세요.\n",
        "\n",
        "4. **데이터 저장 및 관리**\n",
        "   - 수집된 데이터는 개인정보 보호법을 준수하여 관리해야 합니다.\n",
        "   - 민감한 정보가 포함된 경우 적절한 보안 조치를 취하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
