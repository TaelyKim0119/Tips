{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0693cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 자극 키워드 목록 설정\n",
    "2. W 수치가 있는 게시글과 댓글 데이터활용 \n",
    "포포함된 경우 → 해당 키워드들의 K 중 가장 큰 값 사용\n",
    "여러 키워드가 겹치면 max(K_i) 또는 mean(K_i) 적용\n",
    "3. 수식 적용\n",
    "# D 모델\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('C:/Users/speec/OneDrive/Desktop/딥테크팁스/tips/output/Poc2_1.xlsx')\n",
    "df.head()\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # URL 제거\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]\", \"\", text)  # 특수문자 제거 (한글/영문/숫자만 남김)\n",
    "    text = text.lower()\n",
    "    return text.strip()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # 텍스트 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 품사 태깅\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # 명사만 추출 (NN: 명사, NNS: 복수명사, NNP: 고유명사, NNPS: 복수고유명사)\n",
    "    nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "    \n",
    "    # 단어 길이 2 이상 필터링\n",
    "    nouns = [n for n in nouns if len(n) > 1]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "# D output 파일 읽기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 부정 어휘 사전 정의\n",
    "negative_lexicon = [\n",
    "    \"해킹\", \"유출\", \"피해\", \"사고\", \"사건\", \"공격\", \"침해\", \"위험\",\n",
    "    \"불안\", \"불신\", \"불편\", \"문제\", \"대란\", \"먹통\", \"복제\", \"탈취\", \n",
    "    \"도난\", \"사기\", \"의혹\", \"접속대기중\", \"무용지물\", \"부실\", \"늑장\",\n",
    "    \"손해\", \"혼란\", \"충격\", \"비판\", \"비난\", \"무책임\", \"잘못\", \"신세\",\n",
    "    \"방랑\", \"구걸\", \"신고\", \"대처실패\", \"책임전가\", \"보상\", \"과징금\",\n",
    "    \"유심복제\", \"유심탈취\", \"불똥\", \"유심해킹\", \"정보유출\", \"부정확\",\n",
    "    \"지연\", \"우려\", \"접속불가\", \"알림없음\", \"모르면손해\", \"이상현상\",\n",
    "    \"터지다\", \"금융사기\", \"먹통현상\", \"불통\", \"불능\", \"재시작\", \"조치없음\"\n",
    "]\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('C:/Users/speec/OneDrive/Desktop/딥테크팁스/tips/output/Doutput.csv')\n",
    "\n",
    "# merged_tokens에서 부정 어휘 카운트\n",
    "df['negative_count'] = df['merged_tokens'].apply(lambda x: sum(1 for token in eval(x) if token in negative_lexicon))\n",
    "\n",
    "df.head()\n",
    "\n",
    "# D output 파일 읽기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 부정 어휘 사전 정의\n",
    "negative_lexicon = [\n",
    "    \"해킹\", \"유출\", \"피해\", \"사고\", \"사건\", \"공격\", \"침해\", \"위험\",\n",
    "    \"불안\", \"불신\", \"불편\", \"문제\", \"대란\", \"먹통\", \"복제\", \"탈취\", \n",
    "    \"도난\", \"사기\", \"의혹\", \"접속대기중\", \"무용지물\", \"부실\", \"늑장\",\n",
    "    \"손해\", \"혼란\", \"충격\", \"비판\", \"비난\", \"무책임\", \"잘못\", \"신세\",\n",
    "    \"방랑\", \"구걸\", \"신고\", \"대처실패\", \"책임전가\", \"보상\", \"과징금\",\n",
    "    \"유심복제\", \"유심탈취\", \"불똥\", \"유심해킹\", \"정보유출\", \"부정확\",\n",
    "    \"지연\", \"우려\", \"접속불가\", \"알림없음\", \"모르면손해\", \"이상현상\",\n",
    "    \"터지다\", \"금융사기\", \"먹통현상\", \"불통\", \"불능\", \"재시작\", \"조치없음\"\n",
    "]\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('C:/Users/speec/OneDrive/Desktop/딥테크팁스/tips/output/Doutput.csv')\n",
    "\n",
    "# merged_tokens에서 부정 어휘 카운트 및 단어 추출\n",
    "def count_negative_words(tokens):\n",
    "    neg_words = [token for token in eval(tokens) if token in negative_lexicon]\n",
    "    return len(neg_words), neg_words\n",
    "\n",
    "# 부정 단어 개수와 단어 목록 추출\n",
    "df['negative_count'], df['negative_words'] = zip(*df['merged_tokens'].apply(count_negative_words))\n",
    "\n",
    "# 각 행의 부정 단어와 그 개수를 딕셔너리 형태로 저장\n",
    "df['negative_word_counts'] = df['negative_words'].apply(lambda x: dict(pd.Series(x).value_counts()))\n",
    "\n",
    "df.head()\n",
    "\n",
    "lwp_df = pd.read_excel(r'C:\\Users\\speec\\OneDrive\\Desktop\\딥테크팁스\\tips\\LWP_model_results.xlsx')\n",
    "lwp_df.head(1)\n",
    "df['doc_str'] \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. 부정 키워드를 문자열로 연결\n",
    "df['doc_str'] = df['negative_words'].apply(lambda lst: ' '.join(lst))\n",
    "\n",
    "# 2. 6시간 단위로 그룹화하여 문서 생성\n",
    "grouped_docs = df.groupby('time_block')['doc_str'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# 3. TF-IDF 벡터라이저로 변환\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(grouped_docs['doc_str'])\n",
    "\n",
    "# 4. 키워드 이름 추출\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 5. TF-IDF 결과를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=keywords)\n",
    "tfidf_df['time_block'] = grouped_docs['time_block']\n",
    "\n",
    "# 6. 상위 5개 TF-IDF 평균을 D-score로 정의\n",
    "top_k = 3\n",
    "tfidf_df['D_score'] = tfidf_df[keywords].apply(lambda row: sorted(row, reverse=True)[:top_k], axis=1).apply(lambda x: sum(x)/top_k)\n",
    "\n",
    "# 결과 확인\n",
    "display(tfidf_df[['time_block', 'D_score']])\n",
    "\n",
    "# 7. time_block과 D_score만 포함된 데이터프레임을 엑셀로 저장\n",
    "result_df = tfidf_df[['time_block', 'D_score']]\n",
    "result_df.to_excel('C:/Users/speec/OneDrive/Desktop/딥테크팁스/tips/output/D_score_results.xlsx', index=False)\n",
    "\n",
    "lwp_df.head()\n",
    "# Convert time_block to datetime and format to show only up to hours\n",
    "result_df['time_block'] = pd.to_datetime(result_df['time_block']).dt.strftime('%Y-%m-%d %H:00')\n",
    "\n",
    "# Save to df20 and display\n",
    "df20 = result_df.copy()\n",
    "display(df20)\n",
    "\n",
    "\n",
    "df20.head(1)\n",
    "# Convert date_hour to datetime and format to show only up to hours\n",
    "lwp_df['date_hour'] = pd.to_datetime(lwp_df['date_hour']).dt.strftime('%Y-%m-%d %H:00')\n",
    "\n",
    "# Save to df21 and display\n",
    "df21 = lwp_df.copy()\n",
    "df21.head()\n",
    "# Merge df20 and df21 on date_hour and time_block columns\n",
    "merged_df = pd.merge(df20, df21, left_on='time_block', right_on='date_hour', how='inner').dropna()\n",
    "\n",
    "# Save merged dataframe as LWPD model to Excel\n",
    "merged_df.drop('date_hour', axis=1).to_excel('C:/Users/speec/OneDrive/Desktop/딥테크팁스/tips/output/LWPD_model.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "merged_df .columns\n",
    "\n",
    "\n",
    "\n",
    "# Drop date_hour column since it's identical to time_block\n",
    "merged_df = merged_df.drop('date_hour', axis=1)\n",
    "merged_df.info()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
